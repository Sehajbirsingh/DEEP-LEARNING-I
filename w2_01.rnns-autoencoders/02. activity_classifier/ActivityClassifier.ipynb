{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f06df16",
   "metadata": {
    "id": "7f06df16"
   },
   "source": [
    "# Task: Classify human activities based on the sensor data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7ca5e9",
   "metadata": {
    "id": "5c7ca5e9"
   },
   "source": [
    "## Part 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b4c2888",
   "metadata": {
    "id": "5b4c2888"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f045929",
   "metadata": {
    "id": "7f045929"
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b0c0d9",
   "metadata": {
    "id": "c1b0c0d9"
   },
   "outputs": [],
   "source": [
    "data_main = list()\n",
    "\n",
    "dum_data = pd.read_csv('total_acc_x_train.txt', header=None, delim_whitespace=True)\n",
    "data_main.append(dum_data)\n",
    "dum_data = pd.read_csv('total_acc_y_train.txt', header=None, delim_whitespace=True)\n",
    "data_main.append(dum_data)\n",
    "dum_data = pd.read_csv('total_acc_z_train.txt', header=None, delim_whitespace=True)\n",
    "data_main.append(dum_data)\n",
    "\n",
    "dum_data = pd.read_csv('body_acc_x_train.txt', header=None, delim_whitespace=True)\n",
    "data_main.append(dum_data)\n",
    "dum_data = pd.read_csv('body_acc_y_train.txt', header=None, delim_whitespace=True)\n",
    "data_main.append(dum_data)\n",
    "dum_data = pd.read_csv('body_acc_z_train.txt', header=None, delim_whitespace=True)\n",
    "\n",
    "data_main.append(dum_data)\n",
    "dum_data = pd.read_csv('body_gyro_x_train.txt', header=None, delim_whitespace=True)\n",
    "data_main.append(dum_data)\n",
    "dum_data = pd.read_csv('body_gyro_y_train.txt', header=None, delim_whitespace=True)\n",
    "data_main.append(dum_data)\n",
    "dum_data = pd.read_csv('body_gyro_z_train.txt', header=None, delim_whitespace=True)\n",
    "data_main.append(dum_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfd7f29",
   "metadata": {
    "id": "8dfd7f29"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cb4444",
   "metadata": {
    "id": "e0cb4444",
    "outputId": "b7e6aa8c-0bf9-47f0-f543-4ab406366a7f"
   },
   "outputs": [],
   "source": [
    "len(data_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac79b74",
   "metadata": {
    "id": "fac79b74",
    "outputId": "b50dd0f7-3511-4022-fe16-a1911ce54b7f"
   },
   "outputs": [],
   "source": [
    "data_main[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05594b7c",
   "metadata": {
    "id": "05594b7c",
    "outputId": "5ebeec20-590d-4eb5-df24-5e5f02a326a0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_main[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f84b6a0",
   "metadata": {
    "id": "7f84b6a0",
    "outputId": "cc399856-d4a5-4a7b-e732-30a3a730db0c"
   },
   "outputs": [],
   "source": [
    "data_main[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "380e132c",
   "metadata": {
    "id": "380e132c"
   },
   "outputs": [],
   "source": [
    "# make it a 3D aray with featuers\n",
    "data = np.dstack(data_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e64c3",
   "metadata": {
    "id": "c05e64c3",
    "outputId": "daae1103-d7e5-4fe8-f64b-2ca30f4ba66f"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bdb0dc",
   "metadata": {
    "id": "e8bdb0dc"
   },
   "outputs": [],
   "source": [
    "# read the target values\n",
    "y_data = pd.read_csv('y_train.txt', header=None, delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6420da86",
   "metadata": {
    "id": "6420da86",
    "outputId": "69bc96dc-bdff-4201-9bef-f8a4d915b21f"
   },
   "outputs": [],
   "source": [
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9c458b",
   "metadata": {
    "id": "0e9c458b",
    "outputId": "ff9972f8-3d22-4f17-ab68-d53ac224fd25"
   },
   "outputs": [],
   "source": [
    "y_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31f26b1",
   "metadata": {
    "id": "e31f26b1",
    "outputId": "f94a4ff6-3b66-4fbe-9aa6-c738d904554e"
   },
   "outputs": [],
   "source": [
    "y_data.iloc[:, 0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97c44ef2",
   "metadata": {
    "id": "97c44ef2"
   },
   "outputs": [],
   "source": [
    "# Train and test set data\n",
    "x_train = data[0:6000, :, :]\n",
    "y_train = y_data [0:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b470729b",
   "metadata": {
    "id": "b470729b"
   },
   "outputs": [],
   "source": [
    "x_test = data[6000:, :, :]\n",
    "y_test = y_data [6000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcc8b748",
   "metadata": {
    "id": "bcc8b748"
   },
   "outputs": [],
   "source": [
    "# one hot encode the y train data\n",
    "y_train_hot = to_categorical(y_train)\n",
    "y_test_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1347c1",
   "metadata": {
    "id": "3a1347c1",
    "outputId": "8e86a487-f21a-4bbc-afab-cf05e6164477"
   },
   "outputs": [],
   "source": [
    "y_train_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cd1007f",
   "metadata": {
    "id": "7cd1007f"
   },
   "outputs": [],
   "source": [
    "# delete category zero, no such category\n",
    "y_train_hot = y_train_hot[:, 1:7]\n",
    "y_test_hot = y_test_hot[:, 1:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fcf5a51",
   "metadata": {
    "id": "6fcf5a51"
   },
   "outputs": [],
   "source": [
    "# get important sizes and dimensions\n",
    "n_sample = x_train.shape[0]\n",
    "time_steps = x_train.shape[1]\n",
    "n_features = x_train.shape[2]\n",
    "n_outputs = y_train_hot.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3cce1e",
   "metadata": {
    "id": "cc3cce1e",
    "outputId": "4d635a84-9113-4233-9b18-393d7266d66a"
   },
   "outputs": [],
   "source": [
    "# Visualising some of the signal\n",
    "test_sample = 150\n",
    "data_plot = x_train[test_sample, :, 8]\n",
    "plt.plot(data_plot)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('signal value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0cf9b8",
   "metadata": {
    "id": "2b0cf9b8"
   },
   "source": [
    "## Part 2.  Building the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb55597c",
   "metadata": {
    "id": "cb55597c"
   },
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a1bd70",
   "metadata": {
    "id": "17a1bd70"
   },
   "outputs": [],
   "source": [
    "# Initialising the RNN\n",
    "my_classifier = Sequential()\n",
    "\n",
    "# Adding the first LSTM layer and some Dropout regularisation\n",
    "my_classifier.add(LSTM(units = 20, return_sequences = True,\n",
    "                       input_shape = (time_steps, n_features)))\n",
    "my_classifier.add(Dropout(0.2))\n",
    "\n",
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "#my_classifier.add(LSTM(units = 30, return_sequences = True))\n",
    "#my_classifier.add(Dropout(0.2))\n",
    "\n",
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "#my_classifier.add(LSTM(units = 20, return_sequences = True))\n",
    "#my_classifier.add(Dropout(0.2))\n",
    "\n",
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "my_classifier.add(LSTM(units = 20))\n",
    "my_classifier.add(Dropout(0.2))\n",
    "\n",
    "# Adding the output layer\n",
    "my_classifier.add(Dense(units = n_outputs, activation='softmax'))\n",
    "\n",
    "# Compiling the RNN\n",
    "my_classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy',\n",
    "                     metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397ae0c0",
   "metadata": {
    "id": "397ae0c0",
    "outputId": "b95fe086-602c-474c-d42b-e0fd44b36ea5"
   },
   "outputs": [],
   "source": [
    "my_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81258a54",
   "metadata": {
    "id": "81258a54",
    "outputId": "c06aa025-535e-4657-887c-dea073ee1bcc"
   },
   "outputs": [],
   "source": [
    "# Fitting the RNN to the Training set\n",
    "history = my_classifier.fit(x_train, y_train_hot, epochs = 10,\n",
    "                           batch_size = 32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4d6852",
   "metadata": {
    "id": "ba4d6852"
   },
   "source": [
    "## save the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b64844",
   "metadata": {
    "id": "b2b64844"
   },
   "outputs": [],
   "source": [
    "# save the final model\n",
    "my_classifier.save('sig_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07df3af0",
   "metadata": {
    "id": "07df3af0"
   },
   "outputs": [],
   "source": [
    "# to load keras model\n",
    "from keras.models import load_model\n",
    "my_classifier = keras.models.load_model(\"sig_classifier.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1258b4c7",
   "metadata": {
    "id": "1258b4c7"
   },
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da856c2a",
   "metadata": {
    "id": "da856c2a",
    "outputId": "fd7cc2c4-17a8-45e7-edf2-a0110d8f31d0"
   },
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "_, accuracy = my_classifier.evaluate(x_test, y_test_hot, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a1e56e",
   "metadata": {
    "id": "88a1e56e"
   },
   "outputs": [],
   "source": [
    "# Making the predictions and visualising the results\n",
    "y_test_pred_hot = my_classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7457eb6f",
   "metadata": {
    "id": "7457eb6f"
   },
   "outputs": [],
   "source": [
    "#y_test_pred_hot [ y_test_pred_hot>0.5] = 1\n",
    "#y_test_pred_hot [ y_test_pred_hot<0.5] = 0\n",
    "\n",
    "# inverse the \"to_categorical\"\n",
    "y_test_pred = np.argmax(y_test_pred_hot, axis=1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07852c91",
   "metadata": {
    "id": "07852c91"
   },
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c136dce3",
   "metadata": {
    "id": "c136dce3",
    "outputId": "09136cc5-e9f1-41f1-99c3-2f9668a8a09e"
   },
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb625169",
   "metadata": {
    "id": "fb625169",
    "outputId": "50ba7757-ca5b-412c-ac1a-34b6555daeaf"
   },
   "outputs": [],
   "source": [
    "# list all the data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b338cde",
   "metadata": {
    "id": "5b338cde"
   },
   "source": [
    "## Plot the accuracy for both train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10db6ae",
   "metadata": {
    "id": "b10db6ae",
    "outputId": "2f0ac9f5-82d3-4178-eeb0-8357a9852cde"
   },
   "outputs": [],
   "source": [
    "# Plot the accuracy for both train and validation set\n",
    "plt.subplots() # open a new plot\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9742c04",
   "metadata": {
    "id": "e9742c04"
   },
   "source": [
    "## Plot the loss for both train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9210e0",
   "metadata": {
    "id": "cc9210e0",
    "outputId": "b6623e22-edaa-421d-ea92-dde8d6496cbe"
   },
   "outputs": [],
   "source": [
    "# Plot the loss for both train and validation set\n",
    "plt.subplots() # open a new plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d1c1f3",
   "metadata": {
    "id": "c1d1c1f3"
   },
   "source": [
    "## Add more Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ad40a7",
   "metadata": {
    "id": "75ad40a7"
   },
   "outputs": [],
   "source": [
    "# Initialising the RNN\n",
    "my_classifier = Sequential()\n",
    "\n",
    "# Adding the first LSTM layer and some Dropout regularisation\n",
    "my_classifier.add(LSTM(units = 16, return_sequences = True,\n",
    "                       input_shape = (time_steps, n_features)))\n",
    "my_classifier.add(Dropout(0.2))\n",
    "\n",
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "my_classifier.add(LSTM(units = 32, return_sequences = True))\n",
    "my_classifier.add(Dropout(0.2))\n",
    "\n",
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "my_classifier.add(LSTM(units = 16, return_sequences = True))\n",
    "my_classifier.add(Dropout(0.2))\n",
    "\n",
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "my_classifier.add(LSTM(units = 16))\n",
    "my_classifier.add(Dropout(0.2))\n",
    "\n",
    "# Adding the output layer\n",
    "my_classifier.add(Dense(units = n_outputs, activation='softmax'))\n",
    "\n",
    "# Compiling the RNN\n",
    "my_classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy',\n",
    "                     metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c1853b",
   "metadata": {
    "id": "39c1853b",
    "outputId": "0335c884-2c89-48f6-9ada-a4d17164065c"
   },
   "outputs": [],
   "source": [
    "my_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa096a1",
   "metadata": {
    "id": "dfa096a1",
    "outputId": "d58f538e-e7fb-4866-d550-a5c37401e456"
   },
   "outputs": [],
   "source": [
    "# Fitting the RNN to the Training set\n",
    "history = my_classifier.fit(x_train, y_train_hot, epochs = 10,\n",
    "                           batch_size = 32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad9eeba",
   "metadata": {
    "id": "dad9eeba",
    "outputId": "04cbe496-d859-464d-b8cc-76f878e47b83"
   },
   "outputs": [],
   "source": [
    "# Plot the accuracy for both train and validation set\n",
    "plt.subplots() # open a new plot\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d03ae5",
   "metadata": {
    "id": "59d03ae5",
    "outputId": "ba785b69-e3d5-45b0-c388-e8fb2f689a64"
   },
   "outputs": [],
   "source": [
    "# Plot the loss for both train and validation set\n",
    "plt.subplots() # open a new plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7920d3",
   "metadata": {
    "id": "cd7920d3"
   },
   "source": [
    "## Any observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d7e9c7",
   "metadata": {
    "id": "56d7e9c7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
