{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24e244f1",
   "metadata": {},
   "source": [
    "# Task: Car Price Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe2bc50",
   "metadata": {},
   "source": [
    "## Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0240172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfca0032",
   "metadata": {},
   "source": [
    "## Data Set Information:\n",
    "\n",
    "This data set consists of three types of entities: (a) the specification of an auto in terms of various characteristics, (b) its assigned insurance risk rating, (c) its normalized losses in use as compared to other cars. The second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process \"symboling\". A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.\n",
    "\n",
    "The third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc...), and represents the average loss per car per year.\n",
    "\n",
    "Note: Several of the attributes in the database could be used as a \"class\" attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a7feb8",
   "metadata": {},
   "source": [
    "## Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d36408b",
   "metadata": {},
   "source": [
    "1. symboling: -3, -2, -1, 0, 1, 2, 3.\n",
    "2. normalized-losses: continuous from 65 to 256.\n",
    "3. make:\n",
    "alfa-romero, audi, bmw, chevrolet, dodge, honda,\n",
    "isuzu, jaguar, mazda, mercedes-benz, mercury,\n",
    "mitsubishi, nissan, peugot, plymouth, porsche,\n",
    "renault, saab, subaru, toyota, volkswagen, volvo\n",
    "\n",
    "4. fuel-type: diesel, gas.\n",
    "5. aspiration: std, turbo.\n",
    "6. num-of-doors: four, two.\n",
    "7. body-style: hardtop, wagon, sedan, hatchback, convertible.\n",
    "8. drive-wheels: 4wd, fwd, rwd.\n",
    "9. engine-location: front, rear.\n",
    "10. wheel-base: continuous from 86.6 120.9.\n",
    "11. length: continuous from 141.1 to 208.1.\n",
    "12. width: continuous from 60.3 to 72.3.\n",
    "13. height: continuous from 47.8 to 59.8.\n",
    "14. curb-weight: continuous from 1488 to 4066.\n",
    "15. engine-type: dohc, dohcv, l, ohc, ohcf, ohcv, rotor.\n",
    "16. num-of-cylinders: eight, five, four, six, three, twelve, two.\n",
    "17. engine-size: continuous from 61 to 326.\n",
    "18. fuel-system: 1bbl, 2bbl, 4bbl, idi, mfi, mpfi, spdi, spfi.\n",
    "19. bore: continuous from 2.54 to 3.94.\n",
    "20. stroke: continuous from 2.07 to 4.17.\n",
    "21. compression-ratio: continuous from 7 to 23.\n",
    "22. horsepower: continuous from 48 to 288.\n",
    "23. peak-rpm: continuous from 4150 to 6600.\n",
    "24. city-mpg: continuous from 13 to 49.\n",
    "25. highway-mpg: continuous from 16 to 54.\n",
    "26. price: continuous from 5118 to 45400.\t\t\n",
    "\n",
    "**Source: https://archive.ics.uci.edu/ml/datasets/Automobile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b93e47f",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65d1016",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('CarPrice.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e468ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2621f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006c7422",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffe9979",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc1b320",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CarName'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555ced74",
   "metadata": {},
   "source": [
    "## Label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55c4344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27dc6fb",
   "metadata": {},
   "source": [
    "### encode the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487fb9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [feature for feature in df.columns if df[feature].dtype == 'object']\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37de3675",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "\n",
    "for feature in cat_features:\n",
    "    df[feature] = encoder.fit_transform(df[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aefcb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CarName'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1513eb",
   "metadata": {},
   "source": [
    "## Get the x and y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd12f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[:, 1:-1]\n",
    "x = x.drop('CarName', axis = 1)\n",
    "y = pd.DataFrame(df['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6e3f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02432fd0",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397d39f2",
   "metadata": {},
   "source": [
    "### Standard scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7172f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1299ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled = sc.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98ac1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd98779",
   "metadata": {},
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f26b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0865a032",
   "metadata": {},
   "source": [
    "## size/shape of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482dfc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = x_train.shape[0]\n",
    "n_features = x_train.shape[1]\n",
    "print(f'n_samples: {n_samples}, n_features: {n_features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b306611e",
   "metadata": {},
   "source": [
    "## Make the NN using Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9292a395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c6e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Adding the input layer AND the first hidden layer (Pay attention to this)\n",
    "model.add(Dense(units = 16, activation = 'relu', input_dim = n_featuers))\n",
    "\n",
    "# second hidden layer\n",
    "model.add(Dense(units = 8, activation = 'relu'))\n",
    "\n",
    "# Adding the last (output) layer\n",
    "model.add(Dense(units = 1, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014ef90b",
   "metadata": {},
   "source": [
    "## Compiling the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced907c0",
   "metadata": {},
   "source": [
    "### Custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382496d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f07bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"adam\", loss = root_mean_squared_error, \n",
    "              metrics =[tf.keras.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02a6fb4",
   "metadata": {},
   "source": [
    "## Fitting the ANN to the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0962fa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_split=0.2,\n",
    "                            batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6efe49",
   "metadata": {},
   "source": [
    "### list all the data in history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b218985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63874b1e",
   "metadata": {},
   "source": [
    "### Plot the metrics for both train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660656cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy for both train and validation set\n",
    "plt.subplots() # open a new plot\n",
    "plt.plot(history.history['root_mean_squared_error'])\n",
    "plt.plot(history.history['val_root_mean_squared_error'])\n",
    "plt.title('Model RMSE')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2928128",
   "metadata": {},
   "source": [
    "### Plot the loss for both train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511b385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss for both train and validation set\n",
    "plt.subplots() # open a new plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4bbcfe",
   "metadata": {},
   "source": [
    "## Let's try more complex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9393fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128 , activation='relu', input_shape=(n_featuers,))) # Input Layer\n",
    "\n",
    "model.add(Dense(64 , activation='relu'))\n",
    "\n",
    "model.add(Dense(32 , activation='relu'))\n",
    "\n",
    "model.add(Dense(32 , activation='relu'))\n",
    "\n",
    "model.add(Dense(8 , activation='relu'))\n",
    "\n",
    "model.add(Dense(8 , activation='relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam',\n",
    "              loss=root_mean_squared_error,\n",
    "              metrics=[tf.keras.metrics.RootMeanSquaredError()]) # Compiling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd33f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b18e4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, validation_split=0.1,\n",
    "                            batch_size = 64, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de229fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c42474",
   "metadata": {},
   "source": [
    "### Plot the metrics for both train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea8e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the RMSE for both train and validation set\n",
    "plt.subplots()\n",
    "plt.plot(history.history['root_mean_squared_error'])\n",
    "plt.plot(history.history['val_root_mean_squared_error'])\n",
    "plt.title('Model RMSE')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21a1a00",
   "metadata": {},
   "source": [
    "### Plot the loss for both train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a094e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss for both train and validation set\n",
    "plt.subplots() # open a new plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4fb817",
   "metadata": {},
   "source": [
    "## Reduce overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f818d953",
   "metadata": {},
   "source": [
    "### Add L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3227a1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128 , activation='relu', input_shape=(n_featuers,), kernel_regularizer='l2')) \n",
    "\n",
    "model.add(Dense(64 , activation='relu',kernel_regularizer='l2'))\n",
    "\n",
    "model.add(Dense(32 , activation='relu', kernel_regularizer='l2'))\n",
    "\n",
    "model.add(Dense(32 , activation='relu', kernel_regularizer='l2'))\n",
    "\n",
    "model.add(Dense(8 , activation='relu'))\n",
    "\n",
    "model.add(Dense(8 , activation='relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam',\n",
    "              loss=root_mean_squared_error,\n",
    "              metrics=[tf.keras.metrics.RootMeanSquaredError()]) # Compiling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c0d26c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, validation_split=0.1,\n",
    "                            batch_size = 64, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe197796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy for both train and validation set\n",
    "plt.subplots() # open a new plot\n",
    "plt.plot(history.history['root_mean_squared_error'])\n",
    "plt.plot(history.history['val_root_mean_squared_error'])\n",
    "plt.title('model metrics')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7a6766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss for both train and validation set\n",
    "plt.subplots() # open a new plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c63a8e",
   "metadata": {},
   "source": [
    "### Add dropout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1c0c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ab5b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128 , activation='relu', input_shape=(n_featuers,))) \n",
    "model.add(Dropout(0.4)) # Dropout Layer\n",
    "model.add(Dense(64 , activation='relu'))\n",
    "model.add(Dropout(0.4)) # Dropout Layer\n",
    "model.add(Dense(32 , activation='relu'))\n",
    "model.add(Dropout(0.4)) # Dropout Layer\n",
    "model.add(Dense(32 , activation='relu'))\n",
    "model.add(Dropout(0.4)) # Dropout Layer\n",
    "model.add(Dense(8 , activation='relu'))\n",
    "\n",
    "model.add(Dense(8 , activation='relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam',\n",
    "              loss=root_mean_squared_error,\n",
    "              metrics=['mae']) # Compiling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef2549e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, validation_split=0.1,\n",
    "                            batch_size = 128, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609c7d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the metrics for both train and validation set\n",
    "plt.subplots() # open a new plot\n",
    "plt.plot(history.history['mae'])\n",
    "plt.plot(history.history['val_mae'])\n",
    "plt.title('model metrics')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25819e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss for both train and validation set\n",
    "plt.subplots() # open a new plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35471e46",
   "metadata": {},
   "source": [
    "## Make the NN using Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de004aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer Perceptron\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "visible = Input(shape=(n_features,))\n",
    "hidden1 = Dense(128, activation='relu')(visible)\n",
    "hidden2 = Dense(64, activation='relu')(hidden1)\n",
    "hidden3 = Dense(32, activation='relu')(hidden2)\n",
    "hidden4 = Dense(32, activation='relu')(hidden3)\n",
    "output = Dense(1, activation='sigmoid')(hidden4)\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "# summarize layers\n",
    "print(model.summary())\n",
    "# plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89abb82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b7a7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
